# 15.轮询模式驱动程序

DPDK 包括 1 Gigabit、10 Gigabit 和 40 Gigabit 以及半虚拟化 virtio 轮询模式驱动程序。

轮询模式驱动程序 (Poll Mode Driver, PMD) 由 API 组成，通过在用户空间中运行的 BSD 驱动程序提供，用于配置设备及其各自的队列。此外，PMD 直接访问 RX 和 TX 描述符，无需任何中断（链路状态更改中断除外），以便在用户应用程序中快速接收、处理和传送数据包。本节介绍 PMD 的要求、其全局设计原则，并提出以太网 PMD 的高级架构和通用外部 API。

## 15.1.要求和假设

用于数据包处理应用程序的 DPDK 环境允许两种模型：run-to-completion 和 pipe-line：
- 在 run-to-completion 模型中，通过 API 轮询特定端口的 RX 描述符环以获取数据包。然后，数据包在同一核心上进行处理，并通过 API 放置在端口的 TX 描述符环上进行转发。
- 在 pipe-line 模型中，一个核心通过 API 轮询一个或多个端口的 RX 描述符环。数据包被接收并通过环传递到另一个核心。另一个内核继续处理数据包，然后可以通过 API 将其放置在端口的 TX 描述符环上进行转发。

在同步 run-to-completion 模型中，分配给 DPDK 的每个逻辑核心都会执行一个数据包处理循环，其中包括以下步骤：
- 通过 PMD 接收 API 检索输入数据包
- 一次处理每个收到的数据包，直至转发
- 通过 PMD 传输 API 发送待处理的输出数据包

相反，在异步 pipe-line 模型中，一些逻辑核心可以专用于检索接收到的分组，而其他逻辑核心可以专用于处理先前接收到的分组。接收到的数据包通过环在逻辑核心之间进行交换。数据包检索循环包括以下步骤：
- 通过 PMD 接收 API 检索输入数据包 
- 通过数据包队列将接收到的数据包提供给处理lcore

数据包处理循环包括以下步骤：
- 从数据包队列中取出接收到的数据包
- 处理接收到的数据包，直至转发（如果转发）

为了避免任何不必要的中断处理开销，执行环境不得使用任何异步通知机制。只要需要且适当，就应尽可能通过使用环来引入异步通信。

避免锁争用是多核环境中的一个关键问题。为了解决这个问题，PMD 被设计为尽可能多地使用每个核心的私有资源。例如，如果 PMD 不具备 `RTE_ETH_TX_OFFLOAD_MT_LOCKFREE` 功能，则 PMD 会为每个核心、每个端口维护一个单独的传输队列。以同样的方式，端口的每个接收队列都被分配给单个逻辑核心（lcore）并由其轮询。

为了符合非统一内存访问 (NUMA)，内存管理旨在为每个逻辑核心分配本地内存中的专用缓冲池，以最大程度地减少远程内存访问。数据包缓冲池的配置应考虑 DIMM、通道和列方面的底层物理内存架构。应用程序必须确保在创建内存池时给出适当的参数。请参阅 [Mempool 库](https://doc.dpdk.org/guides/prog_guide/mempool_lib.html#mempool-library)。

## 15.2.设计原则

以太网* PMD 的 API 和架构在设计时考虑了以下准则。PMD 必须在上层应用程序级别有助于全局政策导向的决策得到执行。反过来说，NIC PMD 功能不应妨碍上层全局政策所预期的好处，或者更糟的是阻止此类政策的应用。

例如，PMD 的接收和发送功能都具有要轮询的数据包/描述符的最大数量。这允许 run-to-completion 的处理堆栈静态修复或通过不同的全局循环策略动态调整其整体行为，例如：
- 立即接收、处理并以零碎的方式传输一个数据包。
- 接收尽可能多的数据包，然后处理所有接收到的数据包，立即发送它们。
- 接收给定的最大数量的数据包，处理接收到的数据包，累积它们，最后发送所有累积的数据包进行传输。

为了实现最佳性能，必须考虑整体软件设计选择和纯软件优化技术，并与可用的基于硬件的低级优化功能（CPU 缓存属性、总线速度、NIC PCI 带宽等）进行平衡。数据包传输的情况是优化面向突发的网络数据包处理引擎时这种软件/硬件权衡问题的一个例子。数据包传输的一个例子是优化面向突发的网络数据包处理引擎时软件/硬件权衡问题。在初始情况下，PMD 只能导出 rte_eth_tx_one 函数，以便在给定队列上一次传输一个数据包。最重要的是，我们可以轻松构建一个 rte_eth_tx_burst 函数，该函数循环调用 rte_eth_tx_one 函数以一次传输多个数据包。然而，PMD 有效地实现了 rte_eth_tx_burst 函数，通过以下优化来最小化每个数据包的驱动程序级传输成本：
- 在多个数据包之间共享调用 rte_eth_tx_one 函数的未摊销成本
- 启用 rte_eth_tx_burst 函数以利用面向突发的硬件功能（在缓存中预取数据、使用 NIC 头/尾寄存器）来最小化每个数据包的 CPU 周期数，例如，避免对环传输描述符进行不必要的读存储器访问，或者系统地使用完全适合高速缓存行边界和大小的指针数组。
- 应用面向突发的软件优化技术来删除原本不可避免的操作，例如环索引回绕管理。

面向突发的功能也通过 API 引入到 PMD 密集使用的服务中。这尤其适用于用于填充 NIC 环的缓冲区分配器，它提供一次分配/释放多个缓冲区的功能。例如，mbuf_multiple_alloc 函数返回指向 rte_mbuf 缓冲区的指针数组，这会在补充接收环的多个描述符时加速 PMD 的接收轮询函数。

## 15.3.逻辑核心、内存和 NIC 队列关系

DPDK 支持 NUMA，当处理器的逻辑核心和接口利用其本地内存时，可以实现更好的性能。因此，与本地 PCIe* 接口相关的 mbuf 分配应从在本地内存中创建的内存池中分配。如果可能，缓冲区应保留在本地处理器上以获得最佳性能结果，并且 RX 和 TX 缓冲区描述符应使用从本地内存分配的内存池分配的 mbuf 进行填充。

如果数据包或数据操作在本地内存而不是远程处理器内存中，则 run-to-completion 模型也会表现得更好。如果使用的所有逻辑核心都位于同一处理器上，则对于 pipe-line 模型来说也是如此。多个逻辑核心永远不应该共享接口的接收或传输队列，因为这将需要全局锁并影响性能。

如果 PMD 具有 `RTE_ETH_TX_OFFLOAD_MT_LOCKFREE` 功能，则多个线程可以在同一 tx 队列上同时调用 `rte_eth_tx_burst()`，而无需 SW 锁。此 PMD 功能存在于某些 NIC 中，并且在以下用例中非常有用：
- 删除某些应用程序中的显式自旋锁，其中 lcore 未以 1:1 关系映射到 Tx 队列。
- 在 eventdev 用例中，避免为实现更大的扩展性而使用专用的 TX 核心来进行传输，因为所有工作线程都可以发送数据包。

有关 `RTE_ETH_TX_OFFLOAD_MT_LOCKFREE` 功能探测详细信息，请参阅[硬件卸载](https://doc.dpdk.org/guides/prog_guide/poll_mode_drv.html#hardware-offload)。

## 15.4.设备识别、所有权和配置

### 15.4.1.设备识别

每个 NIC 端口均由其（总线/桥接器、设备、功能）PCI 标识符唯一指定，该标识符由 DPDK 初始化时执行的 PCI 探测/枚举函数分配。根据 PCI 标识符，NIC 端口被分配了另外两个标识符：
- 用于指定 PMD API 导出的所有函数中的 NIC 端口的端口索引。
- 用于在控制台消息中指定端口的端口名称，用于管理或调试目的。为了便于使用，端口名称包含端口索引。

### 15.4.2.端口所有权

以太网设备端口可由单个 DPDK 实体（应用程序、库、PMD、进程等）拥有。所有权机制由 ethdev API 控制，并允许 DPDK 实体设置/删除/获取端口所有者。它防止以太网端口由不同的实体管理。

> note:
> DPDK 实体负责在使用端口之前设置端口所有者并管理不同线程或进程之间的端口使用同步。

建议尽早设置端口所有权，例如在探测通知 `RTE_ETH_EVENT_NEW` 期间。

### 15.4.3.设备配置

每个网卡端口的配置包括以下操作：
- 分配 PCI 资源
- 将硬件重置（发出全局重置）到众所周知的默认状态
- 设置 PHY 和链路
- 初始化统计计数器

PMD API 还必须导出用于启动/停止端口的全组播功能的函数以及用于在混杂模式下设置/取消设置端口的函数。某些硬件卸载功能必须在端口初始化时通过特定配置参数单独配置。例如，接收端扩展 (RSS) 和数据中心桥接 (DCB) 功能就是这种情况。

### 15.4.4.即时配置

所有可以“即时”启动或停止（即无需停止设备）的设备功能不需要 PMD API 导出用于此目的的专用函数。

所需要的只是设备 PCI 寄存器的映射地址，以在驱动程序外部的特定函数中实现这些功能的配置。

为此，PMD API 导出一个函数，该函数提供与设备关联的所有信息，这些信息可用于在驱动程序外部设置给定的设备功能。这包括 PCI 供应商标识符、PCI 设备标识符、PCI 设备寄存器的映射地址以及驱动程序的名称。

这种方法的主要优点是，它可以完全自由地选择用于配置、启动和停止此类功能的 API。

作为示例，请参阅 testpmd 应用程序中英特尔® 82576 千兆位以太网控制器和英特尔® 82599 10 千兆位以太网控制器的 IEEE1588 功能配置。

其他特性，如端口的L3/L4五元组包过滤特性，可以用同样的方法配置。可以在各个端口上配置以太网* 流量控制（暂停帧）。详细信息请参考testpmd源代码。此外，只要数据包 mbuf 设置正确，就可以为单个数据包启用 NIC 的 L4 (UDP/TCP/SCTP) 校验和卸载。有关详细信息，请参阅[硬件卸载](https://doc.dpdk.org/guides/prog_guide/poll_mode_drv.html#hardware-offload)。

### 15.4.5.转发队列的配置

每个转发队列独立配置以下信息：

- 转发 ring 描述符数量
- 用于识别适当的 DMA 内存区域的套接字标识符，从中分配 NUMA 架构中的转发 ring
- 转发队列预取、主机和回写阈值寄存器的值
- 空闲阈值 (tx_free_thresh) 的最小转发数据包。当用于传输数据包的描述符数量超过此阈值时，应检查网络适配器是否已写回描述符。在 TX 队列配置期间可以传递值 0，以指示应使用默认值。tx_free_thresh 的默认值为 32。这可确保 PMD 不会搜索已完成的描述符，直到 NIC 已为此队列处理至少 32 个描述符。
- 最小 RS 位阈值。在设置传输描述符中的报告状态 (RS) 位之前要使用的传输描述符的最小数量。请注意，此参数可能仅对 Intel 10 GbE 网络适配器有效。如果自上次 RS 位设置以来使用的描述符数量（直到用于传输数据包的第一个描述符）超过传输 RS 位阈值 (tx_rs_thresh)，则在用于传输数据包的最后一个描述符上设置 RS 位。简而言之，该参数控制网络适配器将哪些传输描述符写回主机内存。在 TX 队列配置期间可以传递值 0，以指示应使用默认值。tx_rs_thresh 的默认值为 32。这可确保在网络适配器写回最近使用的描述符之前至少使用 32 个描述符。这可以节省因 TX 描述符写回而产生的上游 PCIe* 带宽。需要注意的是，当 tx_rs_thresh 大于 1 时，TX 回写阈值 (TX wthresh) 应设置为 0。有关更多详细信息，请参阅英特尔® 82599 10 Gb 以太网控制器数据表。

tx_free_thresh 和 tx_rs_thresh 必须满足以下约束：

- tx_rs_thresh 必须大于 0。 
- tx_rs_thresh 必须小于环的大小减 2。 
- tx_rs_thresh 必须小于或等于 tx_free_thresh。
- tx_free_thresh 必须大于 0。 
- tx_free_thresh 必须小于环的大小减 3。 
- 为了获得最佳性能，当 tx_rs_thresh 大于 1 时，TX wthresh 应设置为 0。

TX 环中的一个描述符用作哨兵以避免硬件竞争条件，从而避免最大阈值约束。

> note:
> 配置 DCB 操作时，在端口初始化时，传输队列数和接收队列数都必须设置为 128。

### 15.4.6.按需释放 Tx mbuf

许多驱动程序不会在数据包传输后立即将 mbuf 释放回内存池或本地缓存。相反，它们将 mbuf 留在 Tx ring 中，并在超过 tx_rs_thresh 时执行批量释放，或者在 Tx ring 中需要时隙时释放 mbuf。

应用程序可以使用 `rte_eth_tx_done_cleanup()` API 请求驱动程序释放已用的 mbuf。此 API 请求驱动程序释放不再使用的 mbuf，无论是否超过 tx_rs_thresh。在两种情况下，应用程序可能希望立即释放 mbuf：
- 当给定数据包需要发送到多个目标接口时（无论是第 2 层泛洪还是第 3 层多播）。
- 有些应用程序被设计为进行多次运行，例如数据包生成器。

要确定驱动程序是否支持此 API，请检查网络接口控制器驱动程序文档中的 Free Tx mbuf ondemand 功能。





